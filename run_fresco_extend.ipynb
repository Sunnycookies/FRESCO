{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import io\n",
    "import gc\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import diffusers\n",
    "import json\n",
    "from typing import List\n",
    "from diffusers import StableDiffusionPipeline, AutoencoderKL, DDPMScheduler, ControlNetModel, DDIMScheduler\n",
    "\n",
    "from src.utils import *\n",
    "from src.keyframe_selection import get_keyframe_ind\n",
    "from src.diffusion_hacked import apply_FRESCO_attn, apply_FRESCO_opt, disable_FRESCO_opt\n",
    "from src.diffusion_hacked import get_flow_and_interframe_paras, get_intraframe_paras, get_flow_and_interframe_paras_warped\n",
    "from src.pipe_FRESCO import inference, inference_extended\n",
    "from src.tokenflow_utils import *\n",
    "from src.pipe_FRESCO import inference_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(config):\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('creating models...')\n",
    "    import sys\n",
    "    sys.path.append(\"./src/ebsynth/deps/gmflow/\")\n",
    "    sys.path.append(\"./src/EGNet/\")\n",
    "    sys.path.append(\"./src/ControlNet/\")\n",
    "    \n",
    "    from gmflow.gmflow import GMFlow\n",
    "    from model import build_model\n",
    "    from annotator.hed import HEDdetector\n",
    "    from annotator.canny import CannyDetector\n",
    "    from annotator.midas import MidasDetector\n",
    "\n",
    "    # optical flow\n",
    "    flow_model = GMFlow(feature_channels=128,\n",
    "                   num_scales=1,\n",
    "                   upsample_factor=8,\n",
    "                   num_head=1,\n",
    "                   attention_type='swin',\n",
    "                   ffn_dim_expansion=4,\n",
    "                   num_transformer_layers=6,\n",
    "                   ).to('cuda')\n",
    "    \n",
    "    checkpoint = torch.load(config['gmflow_path'], map_location=lambda storage, loc: storage)\n",
    "    weights = checkpoint['model'] if 'model' in checkpoint else checkpoint\n",
    "    flow_model.load_state_dict(weights, strict=False)\n",
    "    flow_model.eval() \n",
    "    print('create optical flow estimation model successfully!')\n",
    "    \n",
    "    # saliency detection\n",
    "    sod_model = build_model('resnet')\n",
    "    sod_model.load_state_dict(torch.load(config['sod_path']))\n",
    "    sod_model.to(\"cuda\").eval()\n",
    "    print('create saliency detection model successfully!')\n",
    "    \n",
    "    # controlnet\n",
    "    if config['controlnet_type'] not in ['hed', 'depth', 'canny']:\n",
    "        print('unsupported control type, set to hed')\n",
    "        config['controlnet_type'] = 'hed'\n",
    "    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-\"+config['controlnet_type'], \n",
    "                                                 torch_dtype=torch.float16)\n",
    "    controlnet.to(\"cuda\") \n",
    "    if config['controlnet_type'] == 'depth':\n",
    "        detector = MidasDetector()\n",
    "    elif config['controlnet_type'] == 'canny':\n",
    "        detector = CannyDetector()\n",
    "    else:\n",
    "        detector = HEDdetector()\n",
    "    print('create controlnet model-' + config['controlnet_type'] + ' successfully!')\n",
    "    \n",
    "    # diffusion model\n",
    "    if config['sd_path'] == 'stabilityai/stable-diffusion-2-1-base':\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(config['sd_path'], torch_dtype=torch.float16)\n",
    "    else:\n",
    "        vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16)\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(config['sd_path'], vae=vae, torch_dtype=torch.float16)\n",
    "    pipe.scheduler = DDPMScheduler.from_config(pipe.scheduler.config)\n",
    "    # pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n",
    "    #noise_scheduler = DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "    pipe.to(\"cuda\")\n",
    "    pipe.scheduler.set_timesteps(config['num_inference_steps'], device=pipe._execution_device)\n",
    "    \n",
    "    if config['use_freeu']:\n",
    "        from src.free_lunch_utils import apply_freeu\n",
    "        apply_freeu(pipe, b1=1.2, b2=1.5, s1=1.0, s2=1.0)\n",
    "\n",
    "    if config['use_tokenflow']:\n",
    "        set_tokenflow(pipe.unet, 'SDEdit')\n",
    "\n",
    "    frescoProc = apply_FRESCO_attn(pipe)\n",
    "    frescoProc.controller.disable_controller()\n",
    "    apply_FRESCO_opt(pipe)\n",
    "    print('create diffusion model ' + config['sd_path'] + ' successfully!')\n",
    "    \n",
    "    for param in flow_model.parameters():\n",
    "        param.requires_grad = False    \n",
    "    for param in sod_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in controlnet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in pipe.unet.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    return pipe, frescoProc, controlnet, detector, flow_model, sod_model\n",
    "\n",
    "def apply_control(x, detector, config):\n",
    "    if config['controlnet_type'] == 'depth':\n",
    "        detected_map, _ = detector(x)\n",
    "    elif config['controlnet_type'] == 'canny':\n",
    "        detected_map = detector(x, 50, 100)\n",
    "    else:\n",
    "        detected_map = detector(x)\n",
    "    return detected_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_configs(inputs:List[str] = None, edit_method = 'SDEdit', synth_method = 'Tokenflow', \n",
    "                 use_warp_noise = False, run_ebsynth = False, use_inv_prompts = False,\n",
    "                 use_saliency = True, use_controlnet = True,\n",
    "                 keyframe_select_mode = 'loop', keyframe_select_radix = 6, \n",
    "                 pnp_attn_t = 0.5, pnp_f_t = 0.8, \n",
    "                 inv_step = 500, inv_batch_size = 20, batch_size = 4) -> List[str]:\n",
    "    save_path = '/mnt/netdisk/linjunxin/fresco/'\n",
    "\n",
    "    prompts = '/home/linjx/fresco/data/videos/prompts.json'\n",
    "    inv_prompts = '/home/linjx/TokenFlow/data/tokenflow_supp_videos/inv_prompts.json'\n",
    "    key_intervs = '/home/linjx/fresco/data/videos/key_interv.json'\n",
    "    video_dir = '/home/linjx/fresco/data/videos'\n",
    "    base_model_ref = '/home/linjx/fresco/data/videos/base_model.json'\n",
    "\n",
    "    ref_yaml = '/mnt/netdisk/linjunxin/fresco/ref_config.yaml'\n",
    "    refs = '/home/linjx/fresco/cfg.json'\n",
    "\n",
    "    with open(prompts, 'r') as f:\n",
    "        prompt_dict = json.load(f)\n",
    "\n",
    "    with open(key_intervs, 'r') as f:\n",
    "        key_dict = json.load(f)\n",
    "\n",
    "    if use_inv_prompts == True:\n",
    "        with open(inv_prompts, 'r') as f:\n",
    "            inv_pronpts_dict = json.load(f)\n",
    "\n",
    "    with open(base_model_ref, 'r') as f:\n",
    "        model_dict = json.load(f)\n",
    "\n",
    "    with open(refs, 'r') as f:\n",
    "        cfg_dict = json.load(f)\n",
    "\n",
    "    config_paths = []\n",
    "\n",
    "    for name, prompts in prompt_dict.items():\n",
    "        file_name, ext = os.path.splitext(name)\n",
    "        if inputs is not None and file_name not in inputs:\n",
    "            continue\n",
    "\n",
    "        with open(ref_yaml,'r') as f:\n",
    "            config_yaml = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        config_yaml['file_path'] = os.path.join(video_dir, name)\n",
    "        save_path_video = os.path.join(save_path, f'test-{synth_method}-{edit_method}-{keyframe_select_mode}{'-warp' if use_warp_noise else ''}', file_name)\n",
    "        if not os.path.exists(save_path_video):\n",
    "            os.makedirs(save_path_video)\n",
    "        config_yaml['mininterv'] = key_dict[name][0]\n",
    "        config_yaml['maxinterv'] = key_dict[name][1]\n",
    "        if use_inv_prompts == True:\n",
    "            config_yaml['inv_prompt'] = inv_pronpts_dict[name]\n",
    "        else:\n",
    "            config_yaml['inv_prompt'] = ''\n",
    "        config_yaml['sd_path'] = model_dict[name]\n",
    "        config_yaml['inv_batch_size'] = inv_batch_size\n",
    "        config_yaml['run_ebsynth'] = run_ebsynth\n",
    "        config_yaml['batch_size'] = batch_size\n",
    "        config_yaml['use_tokenflow'] = synth_method == 'Tokenflow'\n",
    "        config_yaml['use_controlnet'] = use_controlnet\n",
    "        config_yaml['edit_mode'] = edit_method\n",
    "        config_yaml['warp_noise'] = use_warp_noise\n",
    "        config_yaml['keyframe_select_mode'] = keyframe_select_mode\n",
    "        config_yaml['keyframe_select_radix'] = keyframe_select_radix\n",
    "        config_yaml['use_saliency'] = use_saliency\n",
    "        config_yaml['num_inference_steps'] = 20\n",
    "        if file_name in cfg_dict:\n",
    "            config_yaml['cond_scale'] = cfg_dict[file_name]['control_scales']\n",
    "            config_yaml['controlnet_type'] = cfg_dict[file_name]['control']\n",
    "            config_yaml['num_warmup_steps'] = int(config_yaml['num_inference_steps'] * cfg_dict[file_name]['strength'])\n",
    "            config_yaml['a_prompt'] = cfg_dict[file_name]['a_prompt']\n",
    "            config_yaml['n_prompt'] = cfg_dict[file_name]['n_prompt']\n",
    "        if use_inv_prompts == True:\n",
    "            inv_path_name = 'latents'\n",
    "        else:\n",
    "            inv_path_name = 'latents-null'\n",
    "\n",
    "        inv_latent_save_path = os.path.join(save_path, inv_path_name, file_name, f'inv_step_{inv_step}')\n",
    "        if not os.path.exists(inv_latent_save_path):\n",
    "            os.makedirs(inv_latent_save_path)\n",
    "        config_yaml['inv_save_path'] = inv_latent_save_path\n",
    "        # inv_latent_load_path = os.path.join(inv_latent_save_path, file_name)\n",
    "        config_yaml['inv_latent_path'] = os.path.join(inv_latent_save_path, 'latents')\n",
    "        \n",
    "        print('=' * 100)\n",
    "        \n",
    "        with open(os.path.join(inv_latent_save_path, 'config.yaml'),'w') as f:\n",
    "            yaml.dump(config_yaml, f, default_flow_style=False)\n",
    "            print(os.path.join(inv_latent_save_path, 'config.yaml'))\n",
    "\n",
    "        with open(os.path.join(save_path_video, 'config.yaml'),'w') as f:\n",
    "            yaml.dump(config_yaml, f, default_flow_style=False)\n",
    "            print(os.path.join(save_path_video, 'config.yaml'))\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            save_video_with_prompts = os.path.join(save_path_video, f'inv_step_{inv_step}', prompt.replace(' ', '_'), \n",
    "                                                f'radix_{keyframe_select_radix}')\n",
    "            if not os.path.exists(save_video_with_prompts):\n",
    "                os.makedirs(save_video_with_prompts)\n",
    "\n",
    "            config_yaml['save_path'] = save_video_with_prompts +'/'\n",
    "            config_yaml['prompt'] = prompt\n",
    "\n",
    "            file_path = os.path.join(save_video_with_prompts, 'config.yaml')\n",
    "            with open(file_path,'w') as f:\n",
    "                yaml.dump(config_yaml,f,default_flow_style=False)\n",
    "                print(file_path)\n",
    "                config_paths.append(file_path)\n",
    "\n",
    "        print('=' * 100)\n",
    "    return config_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['music_input', ]\n",
    "configs = make_configs(inputs=inputs, edit_method='SDEdit', synth_method='Tokenflow',\n",
    "                       use_warp_noise=False, run_ebsynth=False, use_saliency=True,\n",
    "                       use_controlnet=True, keyframe_select_mode='loop', keyframe_select_radix=6)\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Keyframe Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = configs[0]\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "pipe, frescoProc, controlnet, detector, flow_model, sod_model = get_models(config)\n",
    "device = pipe._execution_device\n",
    "guidance_scale = 7.5\n",
    "do_classifier_free_guidance = guidance_scale > 1\n",
    "assert(do_classifier_free_guidance)\n",
    "timesteps = pipe.scheduler.timesteps\n",
    "cond_scale = [config['cond_scale']] * config['num_inference_steps']\n",
    "dilate = Dilate(device=device)\n",
    "\n",
    "base_prompt = config['prompt']\n",
    "if 'Realistic' in config['sd_path'] or 'realistic' in config['sd_path']:\n",
    "    a_prompt = ', RAW photo, subject, (high detailed skin:1.2), 8k uhd, dslr, soft lighting, high quality, film grain, Fujifilm XT3, '\n",
    "    n_prompt = '(deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, mutated hands and fingers:1.4), (deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy, wrong anatomy, extra limb, missing limb, floating limbs, disconnected limbs, mutation, mutated, ugly, disgusting, amputation'\n",
    "else:\n",
    "    a_prompt = ', best quality, extremely detailed, '\n",
    "    n_prompt = 'longbody, lowres, bad anatomy, bad hands, missing finger, extra digit, fewer digits, cropped, worst quality, low quality'    \n",
    "\n",
    "print('\\n' + '=' * 100)\n",
    "print('key frame selection for \\\"%s\\\"...'%(config['file_path']))\n",
    "\n",
    "video_cap = cv2.VideoCapture(config['file_path'])\n",
    "frame_num = int(video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# you can set extra_prompts for individual keyframe\n",
    "# for example, extra_prompts[38] = ', closed eyes' to specify the person frame38 closes the eyes\n",
    "extra_prompts = [''] * frame_num\n",
    "\n",
    "keys = get_keyframe_ind(config['file_path'], frame_num, config['mininterv'], config['maxinterv'],\n",
    "                        mode=config['keyframe_select_mode'], radix=config['keyframe_select_radix'],\n",
    "                        extended=True)\n",
    "sublists_all = []\n",
    "for ind in range(len(keys)):\n",
    "    sublists = [keys[ind][i:i+config['batch_size']-2] for i in range(2, len(keys[ind]), config['batch_size']-2)]\n",
    "    sublists[0].insert(0, keys[ind][0])\n",
    "    sublists[0].insert(1, keys[ind][1])\n",
    "    while len(sublists_all) < len(sublists):\n",
    "        sublists_all.append([])\n",
    "    for batch_ind, keys_batch in enumerate(sublists):\n",
    "        sublists_all[batch_ind].append(keys_batch)\n",
    "print(f\"split keyframes into batches {sublists_all}\")\n",
    "\n",
    "keylists = []\n",
    "for keys_group in sublists_all:\n",
    "    keys_all = []\n",
    "    for key in keys_group:\n",
    "        keys_all += key\n",
    "    keylists.append(keys_all)\n",
    "print(f\"split keyframes into groups {keylists}\")\n",
    "\n",
    "os.makedirs(config['save_path'], exist_ok=True)\n",
    "if os.path.exists(config['save_path']+'keys'):\n",
    "    os.system(f'rm -r {config['save_path']+'keys'}')\n",
    "os.makedirs(config['save_path']+'keys', exist_ok=True)\n",
    "os.makedirs(config['save_path']+'video', exist_ok=True)\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpi = 320\n",
    "\n",
    "batch_ind = 0\n",
    "imgs = []\n",
    "img_idx = []\n",
    "record_latent = []\n",
    "video_cap = cv2.VideoCapture(config['file_path'])\n",
    "for i in range(frame_num):\n",
    "    success, frame = video_cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    img = resize_image(frame, 512)\n",
    "    Image.fromarray(img).save(os.path.join(config['save_path'], 'video/%04d.png'%(i)))\n",
    "    \n",
    "    if i not in keylists[batch_ind] and not config['use_tokenflow']:\n",
    "        continue\n",
    "\n",
    "    imgs += [img]\n",
    "    img_idx += [i]\n",
    "    \n",
    "    if batch_ind < len(keylists) - 1 and i != keylists[batch_ind][-1]:\n",
    "        continue\n",
    "    \n",
    "    if batch_ind == len(keylists) - 1 and config['use_tokenflow'] and i != frame_num - 1:\n",
    "        continue\n",
    "\n",
    "    print(f'processing batch [{batch_ind + 1}/{len(keylists)}] with images {img_idx}')\n",
    "\n",
    "    propagation_mode = batch_ind > 0\n",
    "\n",
    "    prompts = [base_prompt + a_prompt + extra_prompts[ind] for ind in img_idx]\n",
    "    if propagation_mode:\n",
    "        if not config['use_tokenflow']:\n",
    "            assert len(imgs) == len(keylists[batch_ind]) + 2\n",
    "        else:\n",
    "            assert len(imgs) ==  img_idx[-1] - keylists[batch_ind - 1][-1] + 2\n",
    "        prompts = ref_prompts + prompts\n",
    "\n",
    "    print('input of current batch:')\n",
    "    imgs_torch = torch.cat([numpy2tensor(img) for img in imgs], dim=0)\n",
    "    viz = torchvision.utils.make_grid(imgs_torch, len(imgs_torch), 1)\n",
    "    visualize(viz.cpu(), dpi)\n",
    "\n",
    "    edges = torch.cat([numpy2tensor(apply_control(img, detector, config)[:, :, None]) for img in imgs], dim=0)\n",
    "    edges = edges.repeat(1,3,1,1).cuda() * 0.5 + 0.5\n",
    "    if do_classifier_free_guidance:\n",
    "        edges = torch.cat([edges.to(pipe.unet.dtype)] * 2)\n",
    "\n",
    "    pos_map = {img_idx[i]:i for i in range(len(img_idx))}\n",
    "    prefix = [0, 1] if propagation_mode else []\n",
    "    keylists_pos = [prefix + [pos_map[key] for key in keygroup] + ([pos_map[img_idx[-1]]] if img_idx[-1] not in keygroup else [])\n",
    "                    for keygroup in sublists_all[batch_ind]]\n",
    "\n",
    "    print(f'keyframe indexes of images {sublists_all[batch_ind]}')\n",
    "    print(f'keyframe indexes of position {keylists_pos}')\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    latents = inference_extended(pipe, controlnet, frescoProc, imgs, edges, timesteps, keylists_pos, n_prompt,\n",
    "                                 prompts, config['end_opt_step'], config['batch_size'], propagation_mode, False, \n",
    "                                 config['warp_noise'], do_classifier_free_guidance, config['use_tokenflow'], False, \n",
    "                                 config['use_controlnet'], config['use_saliency'], cond_scale, config['num_inference_steps'], \n",
    "                                 config['num_warmup_steps'], config['seed'], guidance_scale, record_latent,\n",
    "                                 flow_model=flow_model, sod_model=sod_model, dilate=dilate)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(dtype=torch.float16, device_type='cuda'):\n",
    "        start = 2 if propagation_mode else 0\n",
    "        size = len(latents)\n",
    "        image = []\n",
    "        for i in range(start, size, config['batch_size']):\n",
    "            end = min(size, i + config['batch_size'])\n",
    "            image_batch = pipe.vae.decode(latents[i:end] / pipe.vae.config.scaling_factor, return_dict=False)[0]\n",
    "            image.append(image_batch)\n",
    "        image = torch.cat(image)\n",
    "        image = torch.clamp(image, -1, 1)\n",
    "        save_imgs = tensor2numpy(image)\n",
    "        for ind, num in enumerate(img_idx[start:]):\n",
    "            Image.fromarray(save_imgs[ind]).save(os.path.join(config['save_path'], 'keys/%04d.png'%(num)))\n",
    "        print('results of current batch:')\n",
    "        viz = torchvision.utils.make_grid(image, len(image), 1)\n",
    "        visualize(viz.cpu(), dpi)\n",
    "\n",
    "    batch_ind += 1\n",
    "    imgs = [imgs[0], imgs[-1]]\n",
    "    img_idx = [img_idx[0], img_idx[-1]]\n",
    "    ref_prompts = [prompts[0], prompts[-1]]\n",
    "    if batch_ind == len(keylists):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_video_multi(roots:List[str], output:str, fps:int = 24, name:str = 'video') -> str:\n",
    "    nfile = len(roots)\n",
    "    img_roots = [os.path.join(root, 'keys') for root in roots]\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "    video_root = os.path.join(output, name + '.mp4')\n",
    "\n",
    "    file_lists = []\n",
    "    for img_root in img_roots:\n",
    "        file_list = os.listdir(img_root)\n",
    "        file_list.sort()\n",
    "        file_lists.append(file_list)\n",
    "    video_len = min([len(file_list) for file_list in file_lists])\n",
    "\n",
    "    ref_frame = cv2.imread(os.path.join(img_roots[0], file_lists[0][0]))\n",
    "    (H, W, C) = ref_frame.shape\n",
    "    if nfile & 1:\n",
    "        size = (nfile * W, H)\n",
    "    else:\n",
    "        size = (2 * W, nfile // 2 * H)\n",
    "\n",
    "    videoWriter = cv2.VideoWriter(video_root, fourcc, fps, size, True)\n",
    "\n",
    "    for i in range(video_len):\n",
    "        frames = []\n",
    "        for j in range(nfile):\n",
    "            frames.append(cv2.imread(os.path.join(img_roots[j], file_lists[j][i])))\n",
    "        if nfile & 1:\n",
    "            frame = cv2.hconcat(frames)\n",
    "        else:\n",
    "            vframes = [cv2.hconcat([frames[k], frames[k+1]]) for k in range(0, nfile, 2)]\n",
    "            frame = cv2.vconcat(vframes)\n",
    "        videoWriter.write(frame)\n",
    "    print('done!\\n')\n",
    "        \n",
    "    videoWriter.release()\n",
    "    return video_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Full Video Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['use_tokenflow']:\n",
    "    keys = [i for i in range(frame_num)]\n",
    "else:\n",
    "    keys = keys[config['num_inference_steps'] % len(keys)]\n",
    "\n",
    "if config['use_tokenflow']:\n",
    "    video_name = config['file_path'].split('/')[-1]\n",
    "    video_name = video_name.split('_')[0]\n",
    "    video_name += f'_{config['edit_mode']}_Tokenflow_{config['keyframe_select_mode']}'\n",
    "    if config['warp_noise']:\n",
    "        video_name += '_warp'\n",
    "    if config['keyframe_select_radix'] == 1:\n",
    "        video_name += '_key'\n",
    "    video_root = to_video_multi(roots=[config['save_path']], output=config['save_path'], \n",
    "                                fps=24, name=video_name)\n",
    "elif not config['run_ebsynth']:\n",
    "    print('to translate full video with ebsynth, install ebsynth and run:')\n",
    "else:\n",
    "    print('translating full video with:')\n",
    "\n",
    "    video_cap = cv2.VideoCapture(config['file_path'])    \n",
    "    fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "    o_video = os.path.join(config['save_path'], 'blend.mp4')\n",
    "    max_process = config['max_process']\n",
    "    save_path = config['save_path']\n",
    "    key_ind = io.StringIO()\n",
    "    for k in keys:\n",
    "        print('%d'%(k), end=' ', file=key_ind)\n",
    "    cmd = (\n",
    "        f'python video_blend.py {save_path} --key keys '\n",
    "        f'--key_ind {key_ind.getvalue()} --output {o_video} --fps {fps} '\n",
    "        f'--n_proc {max_process} -ps')\n",
    "\n",
    "    print('\\n```')\n",
    "    print(cmd)\n",
    "    print('```')\n",
    "\n",
    "    if config['run_ebsynth']:\n",
    "        os.system(cmd)\n",
    "\n",
    "    print('\\n' + '=' * 100)\n",
    "    print('Done') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
